# Amazon ML Challenge 2025 - Price Prediction

## Abstract

This project implements a multimodal machine learning model to predict product prices based on catalog descriptions and product images. The final approach utilizes advanced text embeddings generated by `sentence-transformers`, image features extracted using a pre-trained `EfficientNet-B0` model, and an engineered Item Pack Quantity (IPQ) feature. These features are combined and fed into a `LightGBM` regressor to predict the log-transformed price, which is then converted back to the original price scale for submission.

## Problem Statement

Given product-level inputs (catalog content text and product image URLs), predict the selling price for each product in the provided test set. The objective is to minimize the evaluation metric (e.g., RMSE/MAE or SMAPE) on the holdout test set defined by the challenge organizers.

## Data

* **`train.csv`**: Contains `sample_id`, `catalog_content`, `image_link`, and the target `price`.
* **`test.csv`**: Contains `sample_id`, `catalog_content`, and `image_link`.
* **Product Images**: Downloaded from the `image_link` URLs provided in the CSV files.

No external datasets were used beyond the provided competition data and standard pre-trained models.

## Our Approach

### High-level Components:

1.  **Preprocessing & Feature Engineering**: Extracts key information and prepares data for modeling.
2.  **Text Representation**: Converts catalog descriptions into dense numerical vectors (embeddings).
3.  **Image Representation**: Converts product images into dense numerical vectors (embeddings).
4.  **Modeling**: Trains a gradient boosting model on the combined features.

### Details:

1.  **Preprocessing & Feature Engineering**:
    * **Target Transformation**: The `price` column is log-transformed using `numpy.log1p` to create the `log_price` target variable. This helps stabilize variance and makes the distribution closer to normal.
    * **Item Pack Quantity (IPQ)**: A numeric feature (`ipq`) is extracted from the `catalog_content` using regular expressions to identify patterns like "Pack of X", "Y Count", etc. A default value of 1 is assigned if no pack information is found.

2.  **Text Representation (Advanced)**:
    * The `sentence-transformers` library is used with the `all-MiniLM-L6-v2` pre-trained model to generate 384-dimensional dense embeddings for each product's `catalog_content`.

3.  **Image Representation**:
    * Product images are downloaded using their URLs.
    * The `timm` library is used to load a pre-trained `EfficientNet-B0` model (without its final classification layer).
    * Images are preprocessed (resized, cropped, normalized) according to the `EfficientNet-B0` requirements.
    * The model extracts 1280-dimensional feature vectors (embeddings) for each image. Missing or corrupted images result in zero vectors.

4.  **Model(s)**:
    * **Final Model**: A `LightGBM Regressor` (`LGBMRegressor`) is trained on the horizontally stacked combination of:
        * Advanced Text Embeddings (384 dimensions)
        * IPQ feature (1 dimension)
        * Image Embeddings (1280 dimensions)
    * **Hyperparameters**: `random_state=42`, `n_estimators=500`, `learning_rate=0.05`.
    * **Prediction**: The model predicts `log_price`. The final price predictions are obtained by applying the inverse transformation (`numpy.expm1`) and ensuring no negative prices (setting them to 0).
    * **Baseline (Text Only)**: A baseline model using only TF-IDF text features and IPQ was also implemented using `TfidfVectorizer` (max_features=20000) and `LGBMRegressor`.

## Results

* **Local Validation (SMAPE)**: A local validation using an 80/20 train/validation split (on the full multimodal features including advanced text and image embeddings) achieved a **SMAPE score of approximately 55.65%** on the log-price scale. Note: This validation model was trained only on the 80% split for evaluation purposes. The final submission model is trained on 100% of the training data.
* **Public/Private Leaderboard**: 55

## Hardware & Runtime

* **Development**: Google Colab GPU runtime (Tesla T4 or similar recommended). CPU runtime is possible but extremely slow for image processing and embedding generation.
* **Runtime**:
    * Image Downloading (Train + Test): ~2 hours total.
    * Image Embedding Generation (Train + Test): ~2-2.5 hours total on GPU.
    * Advanced Text Embedding Generation (Train + Test): ~5-10 minutes total on GPU.
    * Model Training (Final LGBM): ~5-10 minutes.

## Reproducibility & Notes

* The notebook sets `random_state=42` for the `LGBMRegressor` for reproducibility.
* Required libraries: `pandas`, `numpy`, `scikit-learn`, `lightgbm`, `torch`, `torchvision`, `timm`, `sentence-transformers`, `Pillow`, `tqdm`. Install `timm` and `sentence-transformers` via pip (`!pip install timm sentence-transformers -q`).
* Ensure sufficient disk space in the Colab environment for downloading all images (~5-10 GB estimate).
* The final submission file generated by the notebook is named **`test_out.csv`** and contains `sample_id` and `price` columns.

## What we tried but did not include (in the final model)

* **TF-IDF Text Features**: A baseline model was built using TF-IDF, but the `sentence-transformers` embeddings provided better performance in the final multimodal setup.

## Acknowledgements

Thanks to the Amazon AI & ML Challenge 2025 organizers for providing the dataset and hosting the competition.
